Interesting data columns for feature engineering

1.MSSubClass may need grouping into a smaller number of categories (convert to categorical)

2.LotFrontage: has missing values that need to be imputed --done

3.Alley: NA should be replaced by a string that says 'None' because it means there is no alley access --done

4.Neighborhood: might need grouping

5.OverallQual: scale should be categorical

6.OverallCond: scale should be categorical

7.YearBuilt: should be categorical. Might need grouping according to range.

8.YearRemodAdd: can be used to create another feature column Remod, which should be 1/0 binary according to the rule YearBuilt==YearRemodAdd then 0 else 1

9.Exterior1st: might need grouping

10.Exterior2nd: might need grouping

Note: both 9 and 10 can be merged together? 

11.MasVnrType: has a few NA that need imputing

12.MasVnrArea: has a few NA that need imputing. What is the area for None ? 0?

13.BsmtQual: NA should be replaced with None. Is there a string 'NA' or a np.null ?

14.BsmtCond: same as 13

15.BsmtExposure: same as 13

16.BsmtFinType1: same as 13

17.BsmtFinSF1: should have 0 corresponding to NA in 16

18.BsmtFinType2: could be used with 16 to create another feature, binary 0/1, BsmtMult, which indicates if there are multiple finished spaces in the basement (1) or not (0)

19.BsmtFinSF2: same as 17

20.BsmtFullBath: should be categorical ? Might need grouping ?

21.BsmtHalfBath: same as 20

22.FullBath: same as 20. 

23.HalfBath: same as 20

24.Bedroom: same as 20

25.Kitchen: same as 20

26.TotRmsAbvGrd: same as 20

27.Fireplaces: same as 20

28.FireplaceQu: check that NA corresponds to 0 in 27. Replace NA by 'None'

29.GarageType: NA to 'None'

30.GarageYrBlt: convert too categorical. Possibly needs grouping. Replace NA with 'None'

31.GarageFinish: same as 29

32.GarageCars: check that 0 corresponds to NA in 29, 30, 31

33.GarageArea: same as 32

34.GarageQual: same as 29

35.GarageCond: same as 29

36.PoolQC: same as 29

37.PoolArea: check that 0 corresponds to NA in 36

38.Fence: same as 29. Mostly NA so may be convert to binary 1/0

39.MiscFeature: same as 38

40.MiscVal: will be mostly 0, should correspond to NA in 39

41.MoSold: should be categorical.

42.YrSold: should be categorical? Probably needs grouping

43.SaleType: might need grouping

44.SaleCondition: same as 43


**Feature Engineering ideas**
*encoding categorical variables*
1. K is better than K-1 binary hot one encoding for the tree based algorithms
2. Split before encoding into train and test to avoid overfitting!
3. Use Feature Engine library, really easy to use to create feature transformers fitted on train set. It also works when there are labels only present in the train set but not the test set. What about labels only in test set? What does it mean to "ignore" them? Can pipelines from scikit learn behave the same? Ignore them means sets all binary variables corresponding to the train set labels to zero. In that sense, K encoding is more clear than k-1 encoding
4. One step that might improve encoding of cat vars with high cardinality is to hot one code only the top labels. This will handle the "new/unseen" variables in test set. Very easy using feature engine
5. Integer encoding is very simple, can be used without expanding the feature space, especially for quick baseline model building. It can actually work well enough for tree based algorithms. Can't handle new unseen labels! Is not suitable for linear models!
6. Another way to encode the categorical variables is to use frequency encoding. This approach works well with tree algorithms, but not suitable for linear algorithms. Also, it does not automatically handle new labels in the test set.
7.Ordered Ordinal Encoding: this approach is suitable for both linear and tree methods. Does not expand the feature space. The issue with it is that it might lead to overfitting and it is tricky to implement with CV currently.
8.Mean encoding replaces each label in a cat var by the mean of the target variable.It has the same advantages and disadvantages as the ordered ordinal encoding.
9.Only for Binary classification --> there is probability ratio encoder
10. Weight of Evidence encoding: also used for binary classification. Primarily used for logistic regression.
11.Rare label encoding/grouping --> can elegantly handle the situation of having high cardinality cat vars and the possibility of brand new labels in test set. Would be interesting to see with and without.
12. Binary encoding and feature hashing

*Variable Transformation*
The main idea is to apply a function on a numerical variable that has a skewed transformation to convert it to an almost normal distribution
1.we have log10, reciprocal, square root, power transformations and box-cox and yeoJohnson transformation
2. With the exception of YeoJohnson and BoxCox, which have a hyper parameter lambda, we can do the transformations before splitting the data into train and test sets.
3. YeoJohnson can be used for all values of x, which is especially useful for variables that have a zero peak. Can we also replace zero with 1 in our house dataset? Or any small positive number ?



*Variable Binning*
1. Equal width binning--> max-min/interval_width

*Engineering Date and Time variables*
1.For my house prices data set, we can create a new variable that is the difference between the house remodelling year and purchase year.
2.Date variables should not be used as categorical variables when building a machine learning model. Not only because they have a multitude of categories, but also because when we actually use the model to score a new observation, this observation will most likely be in the future, an therefore its date label, might be different from the ones contained in the training set and therefore the ones used to train the machine learning algorithm. Therefore a year part for example will be an int numerical variable. This can be binned or left as is.

*Feature Scaling*
1. Tree based models for both classification and regression are robust to features magnitude and do not require feature scaling.


*Assembling the Pipeline*
1. 



